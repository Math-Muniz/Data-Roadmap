<h1 align="center">M√©tricas de Avalia√ß√£o de Modelos</h1>
<h1 align="center">M√©tricas de Avalia√ß√£o de Modelos de Classifica√ß√£o</h1>
<h2 align="center">Matriz de Confus√£o</h2>
<p>Uma forma bastante simples de visualizar a performance de um modelo de classifica√ß√£o √© atrav√©s de uma matriz de confus√£o. Esta matriz indica quantos exemplos existem em cada grupo: falso positivo (FP), falso negativo (FN), verdadeiro positivo (TP) e verdadeiro negativo (TN). √â interessante visualizar a contagem destes grupos tanto em n√∫meros absolutos quanto em porcentagens da classe real, j√° que o n√∫mero de exemplos em cada classe pode variar.</p>
<img src="Matriz-de-Confusao.webp">
<p>A matriz de confus√£o permite visualizar facilmente quantos exemplos foram classificados corretamente e erroneamente em cada classe, que ajuda a entender se o modelo est√° favorecendo uma classe em detrimento da outra. Isto √© importante principalmente em situa√ß√µes em que os erros possuem custos diferentes. Um exemplo disso seria em um modelo para classificar exames de diagn√≥stico de c√¢ncer. Enquanto o erro por falso positivo (classificar uma paciente como doente enquanto ela est√° saud√°vel) seria inconveniente dado que ela demoraria mais a receber alta, o erro por falso negativo (classificar uma paciente como saud√°vel enquanto ela est√° doente) seria bem mais grave, dado que ela poderia receber alta e n√£o ter o acompanhamento necess√°rio.</p>
<p>O exemplo abaixo mostra uma situa√ß√£o em que o erro para a classe positiva √© bem mais alto que para a classe negativa.</p>
<img src="Matriz-de-Confusao2.webp">
<h2 align="center">Acur√°cia (Accuracy)</h2>
<p>A acur√°cia ou accuracy em ingl√™s, nos diz quantos de nossos exemplos foram de fato classificados corretamente, independente da classe. Por exemplo, se temos 100 observa√ß√µes e 90 delas foram classificados corretamente, nosso modelo possui uma acur√°cia de 90%. A acur√°cia √© definida pela f√≥rmula abaixo:</p>
<img src="accuracy.webp">
<p>Na f√≥rmula, podemos ver que esta m√©trica √© definida pela raz√£o entre o que o modelo acertou e todos os exemplos. Apesar da acur√°cia ser uma m√©trica simples, de f√°cil uso e interpret√°vel, ela muitas vezes n√£o √© adequada na pr√°tica.</p>
<p>Uma das maiores desvantagens √© que em alguns problemas a acur√°cia pode ser elevada mas, ainda assim, o modelo pode ter uma performance inadequada. Por exemplo, considere o modelo que classifica exames de c√¢ncer entre positivo ou negativo para a doen√ßa, e em nosso conjunto de dados temos 1000 exemplos, sendo 990 de pacientes sem c√¢ncer e 10 de pacientes com c√¢ncer. Caso nosso modelo seja ing√™nuo e sempre classifique todos os exemplos com negativo (sem c√¢ncer), ele ainda obteria uma acur√°cia de 99%. O que parece uma excelente m√©trica, mas na verdade n√£o estamos avaliando nosso modelo de forma adequada. Para melhor avaliar modelos que lidam com conjuntos de dados desbalanceados como este, outras m√©tricas que ser√£o apresentadas em seguida devem ser utilizadas.</p>
<p>Outro ponto de aten√ß√£o em rela√ß√£o √† acur√°cia √© que ela atribui o mesmo peso para ambos os erros. Por exemplo, considerando o mesmo exemplo de exame de c√¢ncer, suponha que o modelo tenha acertado 950 exemplos do total de 1000 exemplos. Os 50 exemplos errados podem ter sidos da classe positiva (falsos negativos) ou da classe negativa (falsos positivos). Em ambos os casos a acur√°cia seria de 95%, por√©m como mencionado anteriormente, o erro por falso negativo √© bem mais grave neste problema, e isto n√£o √© refletido nesta m√©trica.</p>
<h2 align="center">Precis√£o (Precision)</h2>
<p>A precis√£o ou precision em ingl√™s, tamb√©m √© uma das m√©tricas mais comuns para avaliar modelos de classifica√ß√£o. Esta m√©trica √© definida pela raz√£o entre a quantidade de exemplos classificados corretamente como positivos e o total de exemplos classificados como positivos, conforme a f√≥rmula abaixo:</p>
<img src="precision.webp">
<p>Olhando esta f√≥rmula, podemos ver que a precis√£o d√° um √™nfase maior para os erros por falso positivo. Podemos entender a precis√£o como sendo a express√£o matem√°tica para a pergunta: dos exemplos classificados como positivos, quantos realmente s√£o positivos? Voltando ao exemplo do modelo de c√¢ncer, se o valor para a precis√£o fosse de 90%, isto indicaria que a cada 100 pacientes classificados como positivo, √© esperado que apenas 90 tenham de fato a doen√ßa.</p>
<h2 align="center">Revoca√ß√£o (Recall)</h2>
<p>Ao contr√°rio da precis√£o, a revoca√ß√£o, ou recall em ingl√™s e tamb√©m conhecida como sensibilidade ou taxa de verdadeiro positivo (TPR), d√° maior √™nfase para os erros por falso negativo. Esta m√©trica √© definida pela raz√£o entre a quantidade de exemplos classificados corretamente como positivos e a quantidade de exemplos que s√£o de fato positivos, conforme a f√≥rmula abaixo:</p>
<img src="recall.webp">
<p>A revoca√ß√£o busca responder a seguinte pergunta: de todos os exemplos que s√£o positivos, quantos foram classificados corretamente como positivos? Considerando o exemplo do modelo de c√¢ncer, se o valor para a revoca√ß√£o fosse de 95%, isto indicaria que a cada 100 pacientes que s√£o de fato positivos, √© esperado que apenas 95 sejam corretamente identificados como doentes.</p>
<h2 align="center">M√©trica F1 (F1 Score)</h2>
<p>A m√©trica F1 ou F1 score em ingl√™s e tamb√©m conhecida como F-measure, leva em considera√ß√£o tanto a precis√£o quanto a revoca√ß√£o. Ela √© definida pela m√©dia harm√¥nica entre as duas, como pode ser visto abaixo:</p>
<img src="F1.webp">
<p>Uma das caracter√≠sticas da m√©dia harm√¥nica √© que se a precis√£o ou a revoca√ß√£o for zero ou muito pr√≥ximos disso, o F1-score tamb√©m ser√° baixo. Desta forma, para que o F1-score seja alto, tanto a precis√£o como a revoca√ß√£o tamb√©m devem ser altas. Ou seja, um modelo que apresenta um bom F1-score √© um modelo capaz tanto de acertar suas predi√ß√µes (precis√£o alta) quanto de recuperar os exemplos da classe de interesse (revoca√ß√£o alta). Portanto, esta m√©trica tende a ser um resumo melhor da qualidade do modelo. Uma desvantagem √© que a F1 acaba sendo menos interpret√°vel que a acur√°cia.</p>
<p>No exemplo do modelo de c√¢ncer, se o valor para a precis√£o fosse de 90% e o da revoca√ß√£o fosse de 95%, o valor para a F1 seria de 92.43%.</p>
<h2 align="center">Curva ROC</h2>
<p>Dado um modelo que atribui uma probabilidade para a classe positiva, √© necess√°rio definir um limiar de classifica√ß√£o. Acima deste limiar, um exemplo √© classificado como positivo, caso contr√°rio, √© classificado como negativo. O limiar de classifica√ß√£o influencia o valor das m√©tricas mencionadas anteriormente (acur√°cia, precis√£o, etc), e sua escolha deve levar em considera√ß√£o o custo de cada erro.</p>
<p>A curva ROC (do ingl√™s Receiver Operating Characteristic) pode ser utilizada para avaliar a performance de um classificador para diferentes limiares de classifica√ß√£o. Ela √© constru√≠da medindo a Taxa de Falso Positivo (FPR ‚Äî False Positive Rate) e a Taxa de Verdadeiro Positivo (TPR ‚Äî True Positive Rate) para cada limiar de classifica√ß√£o poss√≠vel, conforme as express√µes abaixo.</p>
<img src="ROC.webp">
<p>A curva ROC √© ent√£o visualizada por meio de um gr√°fico, como mostra o exemplo abaixo. Ela mostra visualmente o compromisso entre falsos positivos e verdadeiro positivos na escolha do limiar. Quanto mais alto o limiar, maior √© taxa de verdadeiro positivo (TP), por√©m a taxa de falso positivo (FP) tamb√©m ser√° maior. No caso extremo em que todos os exemplos s√£o colocados na classe positiva, vemos que ambas as taxas chegam a 100%, enquanto no outro extremo, ambas ficam em 0%. Quanto mais pr√≥xima a curva estiver do canto superior esquerdo, melhor √© a predi√ß√£o do modelo, dado que ele teria 100% de TPR e 0% de FPR. A linha tracejada indica qual seria curva de uma classificador que prev√™ classes de forma aleat√≥ria, e serve como um baseline de compara√ß√£o.</p>
<p>A √°rea sob a curva ROC (AUC ‚Äî Area Under the Curve ou AUROC ‚Äî Area Under the Receiver Operating Characteristic curve) pode ser utilizada como m√©trica de qualidade de um modelo, dado que quanto mais pr√≥xima a curva estiver do canto superior esquerdo, maior ser√° a √°rea sob a curva e melhor ser√° o modelo. Uma vantagem desta m√©trica √© que ela n√£o √© sens√≠vel ao desbalan√ßo de classes, como ocorre com a acur√°cia. Por outro lado, a AUROC n√£o √© t√£o facilmente interpret√°vel.</p>
<img src="ROC-Curve.webp">
<h2 align="center">Perda Logar√≠tmica (Log Loss)</h2>
<h3 align="center">O Que √© Log Loss (Perda Logar√≠tmica) e Como Interpret√°-la?</h3>
<p>Log loss (ou perda logar√≠tmica) √© uma m√©trica de avalia√ß√£o de modelos de classifica√ß√£o em machine learning, tanto bin√°rios quanto multiclasse.</p>
<p>Ela brilha em casos em que √© importante que as probabilidades emitidas pelo modelo estejam alinhadas com a propor√ß√£o real de ocorr√™ncias do alvo.</p>
<p>Para ficar mais claro, imagine que voc√™ esteja criando um modelo para prever se vai chover ou n√£o em um determinado dia.</p>
<p>O modelo faz uma previs√£o em forma de probabilidade, ou seja, ele diz que h√° uma determinada porcentagem de chance de chover no dia.</p>
<p>Otimizando a log loss, voc√™ estar√° otimizando o modelo para que as previs√µes se aproximem da porcentagem real de dias chuvosos.</p>
<p>Por exemplo, se voc√™ pegar 100 dias em que o modelo disse que havia 30% de chance de chuva, em aproximadamente 30 deles ter√° chovido.</p>
<p>Isso √© o que chamamos de modelo calibrado: as probabilidades previstas se aproximam das probabilidades reais dos eventos acontecerem.</p>
<p>Ela √© uma m√©trica negativa, ou seja, quanto menor, melhor. Ela varia entre 0 (previs√µes perfeitas) e ‚àû (previs√µes completamente erradas).</p>
<p>Lembrando que, se seu modelo faz previs√µes perfeitas nos dados de valida√ß√£o, provavelmente tem algo errado na forma como voc√™ est√° avaliando os resultados.üòâ</p>
<h3 align="center">Qual a F√≥rmula da Log Loss?</h3>
<h3 align="center">Classifica√ß√£o Bin√°ria</h3>
<p>A f√≥rmula da log loss para casos de classifica√ß√£o bin√°ria √© a seguinte:</p>
<img src="Log-Loss-Binario.png">
<p>Onde:</p>
<ul>
  <li>n √© o n√∫mero de amostras</li>
  <li>y √© a classe real (0 ou 1)</li>
  <li>p √© a probabilidade prevista pelo modelo (entre 0 e 1)</li>
</ul>
<p>Primeiro voc√™ calcula o valor da log loss para cada exemplo, depois voc√™ faz a m√©dia de todos os exemplos nos quais voc√™ est√° avaliando o modelo e por fim multiplica por -1 para ter um n√∫mero positivo que voc√™ possa minimizar.</p>
<p>Note que a penalidade para previs√µes erradas n√£o √© linear, mas cresce exponencialmente quanto mais confiante o modelo est√° numa previs√£o errada.</p>
<img src="Log-Loss.png">
<p>Se a classe real for positiva e a probabilidade prevista estiver pr√≥xima de zero, a penaliza√ß√£o ser√° enorme para puxar essa previs√£o para longe de zero.</p>
<p>Se a classe real for negativa e a probabilidade prevista estiver pr√≥xima de um, a penaliza√ß√£o tamb√©m ser√° enorme para puxar essa previs√£o na dire√ß√£o do zero.</p>
<h3 align="center">Classifica√ß√£o Multiclasse</h3>
<p>√â muito f√°cil expandi-la para casos em que voc√™ tem mais de duas classes.</p>
<p>N√£o importa quantas classes voc√™ tenha, considere apenas a probabilidade prevista pelo modelo para a classe real de cada exemplo.</p>
<img src="Log-Loss-Multi.png">
<p>Onde:</p>
<ul>
  <li>n √© o n√∫mero de amostras</li>
  <li>y √© igual a 1 para a classe real e 0 para as outras (ou seja, podemos ignorar as probabilidades delas)</li>
  <li>p √© a probabilidade prevista pelo modelo para a classe real da amostra</li>
</ul>
<p>Na pr√°tica, isso significa que basta calcular o logaritmo da probabilidade prevista para a classe real para cada amostra.</p>
<p>Em seguida, voc√™ deve somar todos os valores obtidos e dividir pela quantidade de amostras.</p>
<h3 align="center">Refer√™ncias:</h3>
<p>Trevor Hastie, Robert Tibshirani, Jerome Friedman. 2001. The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc.</p>
<p>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.</p>
<p>Kai Ming Ting. 2011. Encyclopedia of machine learning. Springer. ISBN 978‚Äì0‚Äì387‚Äì30164‚Äì8.</p>
<p>https://medium.com/kunumi/m%C3%A9tricas-de-avalia%C3%A7%C3%A3o-em-machine-learning-classifica%C3%A7%C3%A3o-49340dcdb198</p>
<p>https://mariofilho.com/guia-completo-da-log-loss-perda-logaritmica-em-machine-learning/</p>
