<h1 align="center">M√©tricas de Avalia√ß√£o de Modelos</h1>
<h1 align="center">M√©tricas de Avalia√ß√£o de Modelos de Classifica√ß√£o</h1>
<h2 align="center">Matriz de Confus√£o</h2>
<p>Uma forma bastante simples de visualizar a performance de um modelo de classifica√ß√£o √© atrav√©s de uma matriz de confus√£o. Esta matriz indica quantos exemplos existem em cada grupo: falso positivo (FP), falso negativo (FN), verdadeiro positivo (TP) e verdadeiro negativo (TN). √â interessante visualizar a contagem destes grupos tanto em n√∫meros absolutos quanto em porcentagens da classe real, j√° que o n√∫mero de exemplos em cada classe pode variar.</p>
<img src="Matriz-de-Confusao.webp">
<p>A matriz de confus√£o permite visualizar facilmente quantos exemplos foram classificados corretamente e erroneamente em cada classe, que ajuda a entender se o modelo est√° favorecendo uma classe em detrimento da outra. Isto √© importante principalmente em situa√ß√µes em que os erros possuem custos diferentes. Um exemplo disso seria em um modelo para classificar exames de diagn√≥stico de c√¢ncer. Enquanto o erro por falso positivo (classificar uma paciente como doente enquanto ela est√° saud√°vel) seria inconveniente dado que ela demoraria mais a receber alta, o erro por falso negativo (classificar uma paciente como saud√°vel enquanto ela est√° doente) seria bem mais grave, dado que ela poderia receber alta e n√£o ter o acompanhamento necess√°rio.</p>
<p>O exemplo abaixo mostra uma situa√ß√£o em que o erro para a classe positiva √© bem mais alto que para a classe negativa.</p>
<img src="Matriz-de-Confusao2.webp">
<h2 align="center">Acur√°cia (Accuracy)</h2>
<p>A acur√°cia ou accuracy em ingl√™s, nos diz quantos de nossos exemplos foram de fato classificados corretamente, independente da classe. Por exemplo, se temos 100 observa√ß√µes e 90 delas foram classificados corretamente, nosso modelo possui uma acur√°cia de 90%. A acur√°cia √© definida pela f√≥rmula abaixo:</p>
<img src="accuracy.webp">
<p>Na f√≥rmula, podemos ver que esta m√©trica √© definida pela raz√£o entre o que o modelo acertou e todos os exemplos. Apesar da acur√°cia ser uma m√©trica simples, de f√°cil uso e interpret√°vel, ela muitas vezes n√£o √© adequada na pr√°tica.</p>
<p>Uma das maiores desvantagens √© que em alguns problemas a acur√°cia pode ser elevada mas, ainda assim, o modelo pode ter uma performance inadequada. Por exemplo, considere o modelo que classifica exames de c√¢ncer entre positivo ou negativo para a doen√ßa, e em nosso conjunto de dados temos 1000 exemplos, sendo 990 de pacientes sem c√¢ncer e 10 de pacientes com c√¢ncer. Caso nosso modelo seja ing√™nuo e sempre classifique todos os exemplos com negativo (sem c√¢ncer), ele ainda obteria uma acur√°cia de 99%. O que parece uma excelente m√©trica, mas na verdade n√£o estamos avaliando nosso modelo de forma adequada. Para melhor avaliar modelos que lidam com conjuntos de dados desbalanceados como este, outras m√©tricas que ser√£o apresentadas em seguida devem ser utilizadas.</p>
<p>Outro ponto de aten√ß√£o em rela√ß√£o √† acur√°cia √© que ela atribui o mesmo peso para ambos os erros. Por exemplo, considerando o mesmo exemplo de exame de c√¢ncer, suponha que o modelo tenha acertado 950 exemplos do total de 1000 exemplos. Os 50 exemplos errados podem ter sidos da classe positiva (falsos negativos) ou da classe negativa (falsos positivos). Em ambos os casos a acur√°cia seria de 95%, por√©m como mencionado anteriormente, o erro por falso negativo √© bem mais grave neste problema, e isto n√£o √© refletido nesta m√©trica.</p>
<h2 align="center">Precis√£o (Precision)</h2>
<p>A precis√£o ou precision em ingl√™s, tamb√©m √© uma das m√©tricas mais comuns para avaliar modelos de classifica√ß√£o. Esta m√©trica √© definida pela raz√£o entre a quantidade de exemplos classificados corretamente como positivos e o total de exemplos classificados como positivos, conforme a f√≥rmula abaixo:</p>
<img src="precision.webp">
<p>Olhando esta f√≥rmula, podemos ver que a precis√£o d√° um √™nfase maior para os erros por falso positivo. Podemos entender a precis√£o como sendo a express√£o matem√°tica para a pergunta: dos exemplos classificados como positivos, quantos realmente s√£o positivos? Voltando ao exemplo do modelo de c√¢ncer, se o valor para a precis√£o fosse de 90%, isto indicaria que a cada 100 pacientes classificados como positivo, √© esperado que apenas 90 tenham de fato a doen√ßa.</p>
<h2 align="center">Revoca√ß√£o (Recall)</h2>
<p>Ao contr√°rio da precis√£o, a revoca√ß√£o, ou recall em ingl√™s e tamb√©m conhecida como sensibilidade ou taxa de verdadeiro positivo (TPR), d√° maior √™nfase para os erros por falso negativo. Esta m√©trica √© definida pela raz√£o entre a quantidade de exemplos classificados corretamente como positivos e a quantidade de exemplos que s√£o de fato positivos, conforme a f√≥rmula abaixo:</p>
<img src="recall.webp">
<p>A revoca√ß√£o busca responder a seguinte pergunta: de todos os exemplos que s√£o positivos, quantos foram classificados corretamente como positivos? Considerando o exemplo do modelo de c√¢ncer, se o valor para a revoca√ß√£o fosse de 95%, isto indicaria que a cada 100 pacientes que s√£o de fato positivos, √© esperado que apenas 95 sejam corretamente identificados como doentes.</p>
<h2 align="center">M√©trica F1 (F1 Score)</h2>
<p>A m√©trica F1 ou F1 score em ingl√™s e tamb√©m conhecida como F-measure, leva em considera√ß√£o tanto a precis√£o quanto a revoca√ß√£o. Ela √© definida pela m√©dia harm√¥nica entre as duas, como pode ser visto abaixo:</p>
<img src="F1.webp">
<p>Uma das caracter√≠sticas da m√©dia harm√¥nica √© que se a precis√£o ou a revoca√ß√£o for zero ou muito pr√≥ximos disso, o F1-score tamb√©m ser√° baixo. Desta forma, para que o F1-score seja alto, tanto a precis√£o como a revoca√ß√£o tamb√©m devem ser altas. Ou seja, um modelo que apresenta um bom F1-score √© um modelo capaz tanto de acertar suas predi√ß√µes (precis√£o alta) quanto de recuperar os exemplos da classe de interesse (revoca√ß√£o alta). Portanto, esta m√©trica tende a ser um resumo melhor da qualidade do modelo. Uma desvantagem √© que a F1 acaba sendo menos interpret√°vel que a acur√°cia.</p>
<p>No exemplo do modelo de c√¢ncer, se o valor para a precis√£o fosse de 90% e o da revoca√ß√£o fosse de 95%, o valor para a F1 seria de 92.43%.</p>
<h2 align="center">Curva ROC</h2>
<p>Dado um modelo que atribui uma probabilidade para a classe positiva, √© necess√°rio definir um limiar de classifica√ß√£o. Acima deste limiar, um exemplo √© classificado como positivo, caso contr√°rio, √© classificado como negativo. O limiar de classifica√ß√£o influencia o valor das m√©tricas mencionadas anteriormente (acur√°cia, precis√£o, etc), e sua escolha deve levar em considera√ß√£o o custo de cada erro.</p>
<p>A curva ROC (do ingl√™s Receiver Operating Characteristic) pode ser utilizada para avaliar a performance de um classificador para diferentes limiares de classifica√ß√£o. Ela √© constru√≠da medindo a Taxa de Falso Positivo (FPR ‚Äî False Positive Rate) e a Taxa de Verdadeiro Positivo (TPR ‚Äî True Positive Rate) para cada limiar de classifica√ß√£o poss√≠vel, conforme as express√µes abaixo.</p>
<img src="ROC.webp">
<p>A curva ROC √© ent√£o visualizada por meio de um gr√°fico, como mostra o exemplo abaixo. Ela mostra visualmente o compromisso entre falsos positivos e verdadeiro positivos na escolha do limiar. Quanto mais alto o limiar, maior √© taxa de verdadeiro positivo (TP), por√©m a taxa de falso positivo (FP) tamb√©m ser√° maior. No caso extremo em que todos os exemplos s√£o colocados na classe positiva, vemos que ambas as taxas chegam a 100%, enquanto no outro extremo, ambas ficam em 0%. Quanto mais pr√≥xima a curva estiver do canto superior esquerdo, melhor √© a predi√ß√£o do modelo, dado que ele teria 100% de TPR e 0% de FPR. A linha tracejada indica qual seria curva de uma classificador que prev√™ classes de forma aleat√≥ria, e serve como um baseline de compara√ß√£o.</p>
<p>A √°rea sob a curva ROC (AUC ‚Äî Area Under the Curve ou AUROC ‚Äî Area Under the Receiver Operating Characteristic curve) pode ser utilizada como m√©trica de qualidade de um modelo, dado que quanto mais pr√≥xima a curva estiver do canto superior esquerdo, maior ser√° a √°rea sob a curva e melhor ser√° o modelo. Uma vantagem desta m√©trica √© que ela n√£o √© sens√≠vel ao desbalan√ßo de classes, como ocorre com a acur√°cia. Por outro lado, a AUROC n√£o √© t√£o facilmente interpret√°vel.</p>
<img src="ROC-Curve.webp">
<h2 align="center">Perda Logar√≠tmica (Log Loss)</h2>
<h3 align="center">O Que √© Log Loss (Perda Logar√≠tmica) e Como Interpret√°-la?</h3>
<p>Log loss (ou perda logar√≠tmica) √© uma m√©trica de avalia√ß√£o de modelos de classifica√ß√£o em machine learning, tanto bin√°rios quanto multiclasse.</p>
<p>Ela brilha em casos em que √© importante que as probabilidades emitidas pelo modelo estejam alinhadas com a propor√ß√£o real de ocorr√™ncias do alvo.</p>
<p>Para ficar mais claro, imagine que voc√™ esteja criando um modelo para prever se vai chover ou n√£o em um determinado dia.</p>
<p>O modelo faz uma previs√£o em forma de probabilidade, ou seja, ele diz que h√° uma determinada porcentagem de chance de chover no dia.</p>
<p>Otimizando a log loss, voc√™ estar√° otimizando o modelo para que as previs√µes se aproximem da porcentagem real de dias chuvosos.</p>
<p>Por exemplo, se voc√™ pegar 100 dias em que o modelo disse que havia 30% de chance de chuva, em aproximadamente 30 deles ter√° chovido.</p>
<p>Isso √© o que chamamos de modelo calibrado: as probabilidades previstas se aproximam das probabilidades reais dos eventos acontecerem.</p>
<p>Ela √© uma m√©trica negativa, ou seja, quanto menor, melhor. Ela varia entre 0 (previs√µes perfeitas) e ‚àû (previs√µes completamente erradas).</p>
<p>Lembrando que, se seu modelo faz previs√µes perfeitas nos dados de valida√ß√£o, provavelmente tem algo errado na forma como voc√™ est√° avaliando os resultados.üòâ</p>
<h3 align="center">Qual a F√≥rmula da Log Loss?</h3>
<h3 align="center">Classifica√ß√£o Bin√°ria</h3>
<p>A f√≥rmula da log loss para casos de classifica√ß√£o bin√°ria √© a seguinte:</p>
<img src="Log-Loss-Binario.png">
<p>Onde:</p>
<ul>
  <li>n √© o n√∫mero de amostras</li>
  <li>y √© a classe real (0 ou 1)</li>
  <li>p √© a probabilidade prevista pelo modelo (entre 0 e 1)</li>
</ul>
<p>Primeiro voc√™ calcula o valor da log loss para cada exemplo, depois voc√™ faz a m√©dia de todos os exemplos nos quais voc√™ est√° avaliando o modelo e por fim multiplica por -1 para ter um n√∫mero positivo que voc√™ possa minimizar.</p>
<p>Note que a penalidade para previs√µes erradas n√£o √© linear, mas cresce exponencialmente quanto mais confiante o modelo est√° numa previs√£o errada.</p>
<img src="Log-Loss.png">
<p>Se a classe real for positiva e a probabilidade prevista estiver pr√≥xima de zero, a penaliza√ß√£o ser√° enorme para puxar essa previs√£o para longe de zero.</p>
<p>Se a classe real for negativa e a probabilidade prevista estiver pr√≥xima de um, a penaliza√ß√£o tamb√©m ser√° enorme para puxar essa previs√£o na dire√ß√£o do zero.</p>
<h3 align="center">Classifica√ß√£o Multiclasse</h3>
<p>√â muito f√°cil expandi-la para casos em que voc√™ tem mais de duas classes.</p>
<p>N√£o importa quantas classes voc√™ tenha, considere apenas a probabilidade prevista pelo modelo para a classe real de cada exemplo.</p>
<img src="Log-Loss-Multi.png">
<p>Onde:</p>
<ul>
  <li>n √© o n√∫mero de amostras</li>
  <li>y √© igual a 1 para a classe real e 0 para as outras (ou seja, podemos ignorar as probabilidades delas)</li>
  <li>p √© a probabilidade prevista pelo modelo para a classe real da amostra</li>
</ul>
<p>Na pr√°tica, isso significa que basta calcular o logaritmo da probabilidade prevista para a classe real para cada amostra.</p>
<p>Em seguida, voc√™ deve somar todos os valores obtidos e dividir pela quantidade de amostras.</p>
<h3 align="center">Log Loss Durante o Treino vs Valida√ß√£o</h3>
<p>A Log loss, diferente de outras m√©tricas como acur√°cia e ROC AUC, pode ser usada tanto para otimizar o modelo durante o treino quanto para selecionar hiperpar√¢metros na valida√ß√£o.</p>
<p>Ela √© a m√©trica padr√£o para otimizar modelos de classifica√ß√£o nas bibliotecas mais populares como o scikit-learn.</p>
<p>Durante o treino, a log loss geralmente √© usada com gradiente descendente para ajustar os par√¢metros internos do modelo.</p>
<p>Na valida√ß√£o, ela √© usada para avaliar o desempenho do modelo treinado e selecionar os melhores hiperpar√¢metros.</p>
<p>Ou seja, voc√™ pode us√°-la tanto diretamente na otimiza√ß√£o do modelo sobre os dados de treino quanto na avalia√ß√£o do modelo de treino em dados fora da amostra inicial.</p>
<h1 align="center">M√©tricas de Avalia√ß√£o de Modelos de Regress√£o</h1>
<h2 align="center">R¬≤</h2>
<p>A m√©trica R¬≤, tamb√©m conhecida como <b>R-dois</b> ou <b>coeficiente de determina√ß√£o</b>, representa o percentual da vari√¢ncia dos dados que √© explicado pelo modelo. Os resultados variam de 0 a 1, geralmente tamb√©m s√£o expressos em termos percentuais, ou seja, variando entre 0% e 100%. Quanto maior √© o valor de R¬≤, mais explicativo √© o modelo em rela√ß√£o aos dados previstos. Na equa√ß√£o 2 √© mostrado o c√°lculo desta m√©trica, no qual y e ≈∑ os valores reais e previstos, respectivamente, e y-barra representa a m√©dia dos valores reais.</p>
<img src="R-dois.png">
<p>Por√©m, utilizando somente esta m√©trica, ser√° que um valor de 0,5 j√° seria o suficiente para colocar o modelo em produ√ß√£o? Faz-se necess√°rio aproximar ainda mais o seu resultado para 1? Para responder a essa e outras perguntas, deve-se sempre utilizar outras m√©tricas para se ter uma vis√£o global sobre a performance do modelo.</p>
<h2 align="center">Erro M√©dio Absoluto (MAE ‚Äî Mean Absolute Error)</h2>
<p>O erro m√©dio absoluto (MAE ‚Äî do ingl√™s Mean Absolute Error), como demonstrado na equa√ß√£o 3, mede a m√©dia da diferen√ßa entre o valor real com o predito. Mas por haver valores positivos e negativos, √© adicionado um m√≥dulo entre a diferen√ßa dos valores. Al√©m disso, esta m√©trica n√£o √© afetada por valores discrepantes ‚Äî os denominados outliers.</p>
<img src="MAE.png">
<p>O valor de sa√≠da da equa√ß√£o tem a mesma escala dos dados utilizados para previs√£o, logo fica mais f√°cil a sua interpreta√ß√£o. Se o valor de MAE resultante for igual a 10,01 m, por exemplo, este resultado significa que o modelo pode estar errando em m√©dia 10,01 m para mais quanto para menos em rela√ß√£o ao valor correto. Por isso que para uma previs√£o futura, este resultado precisa ser levado em considera√ß√£o para a tomada de decis√£o. Contudo o quanto este erro representa em rela√ß√£o ao valor real percentualmente?</p>
<h2 align="center">Erro Percentual Absoluto M√©dio (MAPE ‚Äî Mean Absolute Percentual Error)</h2>
<p>O erro percentual absoluto m√©dio (MAPE ‚Äî do ingl√™s Mean Absolute Percentual Error) √© uma m√©trica que mostra a porcentagem de erro em rela√ß√£o aos valores reais. Na equa√ß√£o 4 representa o c√°lculo de MAPE que basicamente se parece com MAE, mas com o acr√©scimo de uma divis√£o por |y|. Ent√£o se o resultado de MAPE for igual a 40% significa que o nosso modelo faz previs√µes que em m√©dia a diferen√ßa entre o valor previsto e o real equivale a 40% do valor real tanto para mais quanto para menos.</p>
<img src="MAPE.png">
<p>Ao observar a equa√ß√£o 4 nota-se que caso o valor de y seja 0, ocorrer√° um erro, devido a divis√£o por zero. Por isso que a biblioteca scikit-learn utiliza a tratativa de colocar um n√∫mero muito pequeno, representado por Œµ, cujo valor √© 2,220446049250313e-16. A m√©trica MAPE √© uma das m√©tricas mais usadas para reportar a performance do modelo, trazendo uma compreens√£o mais abrangente do resultado de MAE.</p>
<h2 align="center">Erro Quadr√°tico M√©dio (MSE ‚Äî Mean Squared Error)</h2>
<p>O erro quadr√°tico m√©dio (MSE ‚Äî do ingl√™s Mean Squared Error) √© uma m√©trica que calcula a m√©dia de diferen√ßa entre o valor predito com o real, como a m√©trica MAE. Entretanto, ao inv√©s de usar o m√≥dulo do resultado entre o valor de y e ≈∑, nesta m√©trica a diferen√ßa √© elevada ao quadrado. Desta maneira penalizando valores que sejam muito diferentes entre o previsto e o real. Portanto, quanto maior √© o valor de MSE, significa que o modelo n√£o performou bem em rela√ß√£o as previs√µes.</p>
<img src="MSE.png">
<p>Apesar de sua ideia poderosa, a m√©trica MSE apresenta um problema de interpretabilidade. Por haver a eleva√ß√£o ao quadrado, a unidade fica distorcida, em outras palavras, se a unidade medida for metros (m), o resultado ser√° em m¬≤. Por isso que uma adapta√ß√£o da MSE √© a RMSE que ser√° apresentada abaixo.</p>
<h2 align="center">Raiz do Erro Quadr√°tico M√©dio (RMSE ‚Äî Root Mean Squared Error)</h2>
<p>A raiz do erro quadr√°tico m√©dio (RMSE ‚Äî do ingl√™s, Root Mean Squared Error) √© basicamente o mesmo c√°lculo de MSE, contendo ainda a mesma ideia de penaliza√ß√£o entre diferen√ßas grandes do valor previsto e o real. Por√©m, para lidar com o problema da diferen√ßa entre unidades, √© aplicada a raiz quadr√°tica como demonstrado na equa√ß√£o 6. Assim a unidade fica na mesma escala que o dado original, resultando em uma melhor interpretabilidade do resultado da m√©trica.</p>
<img src="RMSE.png">
<p>Apesar do valor ter a mesma unidade, ele n√£o costuma se assemelhar ao resultado encontrado de MAE, demonstrando como os outliers podem estar impactando nas previs√µes do modelo. Mas a sua interpretabilidade pode seguir a mesma l√≥gica, onde o resultado da m√©trica sendo igual a 80,0 m, significa que o modelo pode estar errando em 80,0 m para mais ou para menos. Por essa raz√£o, esta m√©trica pode ser uma boa op√ß√£o quando √© preciso ter uma avalia√ß√£o mais criteriosa sobre as previs√µes do modelo.</p>
<h3 align="center">Refer√™ncias:</h3>
<p>Trevor Hastie, Robert Tibshirani, Jerome Friedman. 2001. The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc.</p>
<p>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.</p>
<p>Kai Ming Ting. 2011. Encyclopedia of machine learning. Springer. ISBN 978‚Äì0‚Äì387‚Äì30164‚Äì8.</p>
<p>https://medium.com/kunumi/m%C3%A9tricas-de-avalia%C3%A7%C3%A3o-em-machine-learning-classifica%C3%A7%C3%A3o-49340dcdb198</p>
<p>https://mariofilho.com/guia-completo-da-log-loss-perda-logaritmica-em-machine-learning/</p>
<p>https://medium.com/data-hackers/prevendo-n%C3%BAmeros-entendendo-m%C3%A9tricas-de-regress%C3%A3o-35545e011e70</p>
