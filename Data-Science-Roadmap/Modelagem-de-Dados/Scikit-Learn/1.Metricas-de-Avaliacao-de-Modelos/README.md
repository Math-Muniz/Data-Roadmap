<h1 align="center">M√©tricas de Avalia√ß√£o de Modelos</h1>
<h1 align="center">M√©tricas de Avalia√ß√£o de Modelos de Classifica√ß√£o</h1>
<h2 align="center">Matriz de Confus√£o</h2>
<p>Uma forma bastante simples de visualizar a performance de um modelo de classifica√ß√£o √© atrav√©s de uma matriz de confus√£o. Esta matriz indica quantos exemplos existem em cada grupo: falso positivo (FP), falso negativo (FN), verdadeiro positivo (TP) e verdadeiro negativo (TN). √â interessante visualizar a contagem destes grupos tanto em n√∫meros absolutos quanto em porcentagens da classe real, j√° que o n√∫mero de exemplos em cada classe pode variar.</p>
<img src="Matriz-de-Confusao.webp">
<p>A matriz de confus√£o permite visualizar facilmente quantos exemplos foram classificados corretamente e erroneamente em cada classe, que ajuda a entender se o modelo est√° favorecendo uma classe em detrimento da outra. Isto √© importante principalmente em situa√ß√µes em que os erros possuem custos diferentes. Um exemplo disso seria em um modelo para classificar exames de diagn√≥stico de c√¢ncer. Enquanto o erro por falso positivo (classificar uma paciente como doente enquanto ela est√° saud√°vel) seria inconveniente dado que ela demoraria mais a receber alta, o erro por falso negativo (classificar uma paciente como saud√°vel enquanto ela est√° doente) seria bem mais grave, dado que ela poderia receber alta e n√£o ter o acompanhamento necess√°rio.</p>
<p>O exemplo abaixo mostra uma situa√ß√£o em que o erro para a classe positiva √© bem mais alto que para a classe negativa.</p>
<img src="Matriz-de-Confusao2.webp">
<h2 align="center">Acur√°cia (Accuracy)</h2>
<p>A acur√°cia ou accuracy em ingl√™s, nos diz quantos de nossos exemplos foram de fato classificados corretamente, independente da classe. Por exemplo, se temos 100 observa√ß√µes e 90 delas foram classificados corretamente, nosso modelo possui uma acur√°cia de 90%. A acur√°cia √© definida pela f√≥rmula abaixo:</p>
<img src="accuracy.webp">
<p>Na f√≥rmula, podemos ver que esta m√©trica √© definida pela raz√£o entre o que o modelo acertou e todos os exemplos. Apesar da acur√°cia ser uma m√©trica simples, de f√°cil uso e interpret√°vel, ela muitas vezes n√£o √© adequada na pr√°tica.</p>
<p>Uma das maiores desvantagens √© que em alguns problemas a acur√°cia pode ser elevada mas, ainda assim, o modelo pode ter uma performance inadequada. Por exemplo, considere o modelo que classifica exames de c√¢ncer entre positivo ou negativo para a doen√ßa, e em nosso conjunto de dados temos 1000 exemplos, sendo 990 de pacientes sem c√¢ncer e 10 de pacientes com c√¢ncer. Caso nosso modelo seja ing√™nuo e sempre classifique todos os exemplos com negativo (sem c√¢ncer), ele ainda obteria uma acur√°cia de 99%. O que parece uma excelente m√©trica, mas na verdade n√£o estamos avaliando nosso modelo de forma adequada. Para melhor avaliar modelos que lidam com conjuntos de dados desbalanceados como este, outras m√©tricas que ser√£o apresentadas em seguida devem ser utilizadas.</p>
<p>Outro ponto de aten√ß√£o em rela√ß√£o √† acur√°cia √© que ela atribui o mesmo peso para ambos os erros. Por exemplo, considerando o mesmo exemplo de exame de c√¢ncer, suponha que o modelo tenha acertado 950 exemplos do total de 1000 exemplos. Os 50 exemplos errados podem ter sidos da classe positiva (falsos negativos) ou da classe negativa (falsos positivos). Em ambos os casos a acur√°cia seria de 95%, por√©m como mencionado anteriormente, o erro por falso negativo √© bem mais grave neste problema, e isto n√£o √© refletido nesta m√©trica.</p>
<h2 align="center">Precis√£o (Precision)</h2>
<p>A precis√£o ou precision em ingl√™s, tamb√©m √© uma das m√©tricas mais comuns para avaliar modelos de classifica√ß√£o. Esta m√©trica √© definida pela raz√£o entre a quantidade de exemplos classificados corretamente como positivos e o total de exemplos classificados como positivos, conforme a f√≥rmula abaixo:</p>
<img src="precision.webp">
<p>Olhando esta f√≥rmula, podemos ver que a precis√£o d√° um √™nfase maior para os erros por falso positivo. Podemos entender a precis√£o como sendo a express√£o matem√°tica para a pergunta: dos exemplos classificados como positivos, quantos realmente s√£o positivos? Voltando ao exemplo do modelo de c√¢ncer, se o valor para a precis√£o fosse de 90%, isto indicaria que a cada 100 pacientes classificados como positivo, √© esperado que apenas 90 tenham de fato a doen√ßa.</p>
<h2 align="center">Revoca√ß√£o (Recall)</h2>
<p>Ao contr√°rio da precis√£o, a revoca√ß√£o, ou recall em ingl√™s e tamb√©m conhecida como sensibilidade ou taxa de verdadeiro positivo (TPR), d√° maior √™nfase para os erros por falso negativo. Esta m√©trica √© definida pela raz√£o entre a quantidade de exemplos classificados corretamente como positivos e a quantidade de exemplos que s√£o de fato positivos, conforme a f√≥rmula abaixo:</p>
<img src="recall.webp">
<p>A revoca√ß√£o busca responder a seguinte pergunta: de todos os exemplos que s√£o positivos, quantos foram classificados corretamente como positivos? Considerando o exemplo do modelo de c√¢ncer, se o valor para a revoca√ß√£o fosse de 95%, isto indicaria que a cada 100 pacientes que s√£o de fato positivos, √© esperado que apenas 95 sejam corretamente identificados como doentes.</p>
<h2 align="center">M√©trica F1 (F1 Score)</h2>
<p>A m√©trica F1 ou F1 score em ingl√™s e tamb√©m conhecida como F-measure, leva em considera√ß√£o tanto a precis√£o quanto a revoca√ß√£o. Ela √© definida pela m√©dia harm√¥nica entre as duas, como pode ser visto abaixo:</p>
<img src="F1.webp">
<p>Uma das caracter√≠sticas da m√©dia harm√¥nica √© que se a precis√£o ou a revoca√ß√£o for zero ou muito pr√≥ximos disso, o F1-score tamb√©m ser√° baixo. Desta forma, para que o F1-score seja alto, tanto a precis√£o como a revoca√ß√£o tamb√©m devem ser altas. Ou seja, um modelo que apresenta um bom F1-score √© um modelo capaz tanto de acertar suas predi√ß√µes (precis√£o alta) quanto de recuperar os exemplos da classe de interesse (revoca√ß√£o alta). Portanto, esta m√©trica tende a ser um resumo melhor da qualidade do modelo. Uma desvantagem √© que a F1 acaba sendo menos interpret√°vel que a acur√°cia.</p>
<p>No exemplo do modelo de c√¢ncer, se o valor para a precis√£o fosse de 90% e o da revoca√ß√£o fosse de 95%, o valor para a F1 seria de 92.43%.</p>
<h2 align="center">Curva ROC</h2>
<p>Dado um modelo que atribui uma probabilidade para a classe positiva, √© necess√°rio definir um limiar de classifica√ß√£o. Acima deste limiar, um exemplo √© classificado como positivo, caso contr√°rio, √© classificado como negativo. O limiar de classifica√ß√£o influencia o valor das m√©tricas mencionadas anteriormente (acur√°cia, precis√£o, etc), e sua escolha deve levar em considera√ß√£o o custo de cada erro.</p>
<p>A curva ROC (do ingl√™s Receiver Operating Characteristic) pode ser utilizada para avaliar a performance de um classificador para diferentes limiares de classifica√ß√£o. Ela √© constru√≠da medindo a Taxa de Falso Positivo (FPR ‚Äî False Positive Rate) e a Taxa de Verdadeiro Positivo (TPR ‚Äî True Positive Rate) para cada limiar de classifica√ß√£o poss√≠vel, conforme as express√µes abaixo.</p>
<img src="ROC.webp">
<p>A curva ROC √© ent√£o visualizada por meio de um gr√°fico, como mostra o exemplo abaixo. Ela mostra visualmente o compromisso entre falsos positivos e verdadeiro positivos na escolha do limiar. Quanto mais alto o limiar, maior √© taxa de verdadeiro positivo (TP), por√©m a taxa de falso positivo (FP) tamb√©m ser√° maior. No caso extremo em que todos os exemplos s√£o colocados na classe positiva, vemos que ambas as taxas chegam a 100%, enquanto no outro extremo, ambas ficam em 0%. Quanto mais pr√≥xima a curva estiver do canto superior esquerdo, melhor √© a predi√ß√£o do modelo, dado que ele teria 100% de TPR e 0% de FPR. A linha tracejada indica qual seria curva de uma classificador que prev√™ classes de forma aleat√≥ria, e serve como um baseline de compara√ß√£o.</p>
<p>A √°rea sob a curva ROC (AUC ‚Äî Area Under the Curve ou AUROC ‚Äî Area Under the Receiver Operating Characteristic curve) pode ser utilizada como m√©trica de qualidade de um modelo, dado que quanto mais pr√≥xima a curva estiver do canto superior esquerdo, maior ser√° a √°rea sob a curva e melhor ser√° o modelo. Uma vantagem desta m√©trica √© que ela n√£o √© sens√≠vel ao desbalan√ßo de classes, como ocorre com a acur√°cia. Por outro lado, a AUROC n√£o √© t√£o facilmente interpret√°vel.</p>
<img src="ROC-Curve.webp">
<h2 align="center">Perda Logar√≠tmica (Log Loss)</h2>
<h3 align="center">O Que √© Log Loss (Perda Logar√≠tmica) e Como Interpret√°-la?</h3>
<p>Log loss (ou perda logar√≠tmica) √© uma m√©trica de avalia√ß√£o de modelos de classifica√ß√£o em machine learning, tanto bin√°rios quanto multiclasse.</p>
<p>Ela brilha em casos em que √© importante que as probabilidades emitidas pelo modelo estejam alinhadas com a propor√ß√£o real de ocorr√™ncias do alvo.</p>
<p>Para ficar mais claro, imagine que voc√™ esteja criando um modelo para prever se vai chover ou n√£o em um determinado dia.</p>
<p>O modelo faz uma previs√£o em forma de probabilidade, ou seja, ele diz que h√° uma determinada porcentagem de chance de chover no dia.</p>
<p>Otimizando a log loss, voc√™ estar√° otimizando o modelo para que as previs√µes se aproximem da porcentagem real de dias chuvosos.</p>
<p>Por exemplo, se voc√™ pegar 100 dias em que o modelo disse que havia 30% de chance de chuva, em aproximadamente 30 deles ter√° chovido.</p>
<p>Isso √© o que chamamos de modelo calibrado: as probabilidades previstas se aproximam das probabilidades reais dos eventos acontecerem.</p>
<p>Ela √© uma m√©trica negativa, ou seja, quanto menor, melhor. Ela varia entre 0 (previs√µes perfeitas) e ‚àû (previs√µes completamente erradas).</p>
<p>Lembrando que, se seu modelo faz previs√µes perfeitas nos dados de valida√ß√£o, provavelmente tem algo errado na forma como voc√™ est√° avaliando os resultados.üòâ</p>
<h3 align="center">Qual a F√≥rmula da Log Loss?</h3>
<h3 align="center">Classifica√ß√£o Bin√°ria</h3>
<p>A f√≥rmula da log loss para casos de classifica√ß√£o bin√°ria √© a seguinte:</p>
<img src="Log-Loss-Binario.png">
<p>Onde:</p>
<ul>
  <li>n √© o n√∫mero de amostras</li>
  <li>y √© a classe real (0 ou 1)</li>
  <li>p √© a probabilidade prevista pelo modelo (entre 0 e 1)</li>
</ul>
<p>Primeiro voc√™ calcula o valor da log loss para cada exemplo, depois voc√™ faz a m√©dia de todos os exemplos nos quais voc√™ est√° avaliando o modelo e por fim multiplica por -1 para ter um n√∫mero positivo que voc√™ possa minimizar.</p>
<p>Note que a penalidade para previs√µes erradas n√£o √© linear, mas cresce exponencialmente quanto mais confiante o modelo est√° numa previs√£o errada.</p>
<img src="Log-Loss.png">
<p>Se a classe real for positiva e a probabilidade prevista estiver pr√≥xima de zero, a penaliza√ß√£o ser√° enorme para puxar essa previs√£o para longe de zero.</p>
<p>Se a classe real for negativa e a probabilidade prevista estiver pr√≥xima de um, a penaliza√ß√£o tamb√©m ser√° enorme para puxar essa previs√£o na dire√ß√£o do zero.</p>
<h3 align="center">Classifica√ß√£o Multiclasse</h3>
<p>√â muito f√°cil expandi-la para casos em que voc√™ tem mais de duas classes.</p>
<p>N√£o importa quantas classes voc√™ tenha, considere apenas a probabilidade prevista pelo modelo para a classe real de cada exemplo.</p>
<img src="Log-Loss-Multi.png">
<p>Onde:</p>
<ul>
  <li>n √© o n√∫mero de amostras</li>
  <li>y √© igual a 1 para a classe real e 0 para as outras (ou seja, podemos ignorar as probabilidades delas)</li>
  <li>p √© a probabilidade prevista pelo modelo para a classe real da amostra</li>
</ul>
<p>Na pr√°tica, isso significa que basta calcular o logaritmo da probabilidade prevista para a classe real para cada amostra.</p>
<p>Em seguida, voc√™ deve somar todos os valores obtidos e dividir pela quantidade de amostras.</p>
<h3 align="center">Log Loss Durante o Treino vs Valida√ß√£o</h3>
<p>A Log loss, diferente de outras m√©tricas como acur√°cia e ROC AUC, pode ser usada tanto para otimizar o modelo durante o treino quanto para selecionar hiperpar√¢metros na valida√ß√£o.</p>
<p>Ela √© a m√©trica padr√£o para otimizar modelos de classifica√ß√£o nas bibliotecas mais populares como o scikit-learn.</p>
<p>Durante o treino, a log loss geralmente √© usada com gradiente descendente para ajustar os par√¢metros internos do modelo.</p>
<p>Na valida√ß√£o, ela √© usada para avaliar o desempenho do modelo treinado e selecionar os melhores hiperpar√¢metros.</p>
<p>Ou seja, voc√™ pode us√°-la tanto diretamente na otimiza√ß√£o do modelo sobre os dados de treino quanto na avalia√ß√£o do modelo de treino em dados fora da amostra inicial.</p>
<h2 align="center">Kolmogorov-Smirnov (KS)</h2>
<h3 align="center">O que √© o Teste de Kolmogorov-Smirnov (teste KS ou teste K-S)?</h3>
<p>O teste de Kolmogorov-Smirnov (KS test ou teste K-S) √© usado para comparar duas distribui√ß√µes a fim de determinar se elas prov√™m da mesma distribui√ß√£o subjacente.</p>
<p>No caso de uso t√≠pico em aprendizado de m√°quina, existem duas distribui√ß√µes (A e B) que voc√™ est√° tentando comparar. Uma distribui√ß√£o A pode ser uma caracter√≠stica, como saldo da conta, no conjunto de dados de treinamento do modelo. A outra distribui√ß√£o B pode ser a mesma caracter√≠stica, saldo da conta, mas agregada em um per√≠odo de tempo de produ√ß√£o (tipicamente di√°rio ou por hora). Como construtor de modelos, voc√™ deseja saber se o modelo implantado est√° sendo usado em uma distribui√ß√£o de dados diferente da do conjunto de treinamento. O teste de Kolmogorov-Smirnov (KS) √© um teste estat√≠stico n√£o param√©trico que pode ser usado para comparar A e B e ver se s√£o diferentes.</p>
<p>Ao contr√°rio de outras m√©tricas que permitem comparar distribui√ß√µes, como a diverg√™ncia de Jensen-Shannon (JS), PSI e a diverg√™ncia de Kullback-Leibler (KL), que se baseiam na teoria da informa√ß√£o de Claude Shannon, o teste K-S deriva da estat√≠stica. √â n√£o param√©trico, o que significa que funciona com todos os tipos de distribui√ß√µes. Portanto, n√£o est√° limitado ao uso em distribui√ß√µes bem conhecidas, como a normal ou binomial - seus dados podem ter qualquer forma e o teste K-S ainda funcionar√°. Vale ressaltar que a estat√≠stica do teste K-S tamb√©m n√£o √© uma m√©trica no sentido formal, pois n√£o atende √† desigualdade triangular.</p>
<p>Em aplica√ß√µes de produ√ß√£o de aprendizado de m√°quina, as caracter√≠sticas podem ser de todos os tipos de distribui√ß√µes, portanto, o uso de uma estat√≠stica n√£o param√©trica √© um requisito para a an√°lise. A estat√≠stica KS √© limitada em suas aplica√ß√µes a vari√°veis num√©ricas, como caracter√≠sticas que s√£o valores de ponto flutuante ou inteiros, e n√£o funciona para caracter√≠sticas categ√≥ricas discretas.</p>
<h3 align="center">Vis√£o Geral da Estat√≠stica do Teste K-S (Teste de Kolmogorov-Smirnov)</h3>
<p>O objetivo do teste K-S √© determinar se duas distribui√ß√µes, A e B, s√£o diferentes. A estat√≠stica do teste K-S fornece um valor num√©rico relacionado a essa diferen√ßa. A estat√≠stica do teste K-S √© definida como o valor m√°ximo da diferen√ßa entre as fun√ß√µes de distribui√ß√£o acumulada (CDF) de A e B. Em configura√ß√µes de aprendizado de m√°quina, as CDFs s√£o normalmente derivadas empiricamente a partir de amostras dos conjuntos de dados e seriam chamadas de eCDFs.</p>
<img src="histogram-versus-smooth.webp">
<p>Os dados que voc√™ coleta para uma vari√°vel ou caracter√≠stica, seja no treinamento ou na produ√ß√£o, podem se parecer com as distribui√ß√µes acima. Uma eCDF emp√≠rica √© criada ordenando os dados por valor e criando um gr√°fico em peda√ßos.</p>
<img src="ks-test-math.webp">
<p>A CDF cria uma soma cumulativa em etapas ao longo da faixa da vari√°vel de dados.</p>
<img src="ks-statistic-example-home-loans.webp">
<p>No exemplo acima, voc√™ pode ver a eCDF da caracter√≠stica saldo da conta da produ√ß√£o em compara√ß√£o com a eCDF do saldo da conta do treinamento. A eCDF em peda√ßos azuis representa os dados de treinamento, enquanto a linha preta representa os dados de produ√ß√£o. A seta vermelha representa a estat√≠stica do teste K-S, que √© a diferen√ßa m√°xima entre as eCDFs.</p>
<img src="k-s-test-equation-math.webp">
<img src="ks-test-statistic-math-2.webp">
<h3 align="center">Qual √© a Diferen√ßa Entre o Teste K-S de Uma Amostra e o Teste K-S de Duas Amostras?</h3>
<p>Existem duas vers√µes do teste K-S, o de uma amostra e o de duas amostras (n√£o deve ser confundido com um lado e dois lados). Os exemplos acima s√£o do teste K-S de duas amostras, em que ambas as distribui√ß√µes s√£o emp√≠ricas e derivadas de dados reais.</p>
<ul>
  <li>Dois Amostras: eCDF de A comparada com eCDF de B</li>
  <li>Uma Amostra: eCDF de A comparada com CDF de B</li>
</ul>
<p>O teste K-S de uma amostra √© quase nunca usado na produ√ß√£o de aprendizado de m√°quina. O teste K-S de uma amostra √© usado quando voc√™ est√° comparando uma √∫nica amostra emp√≠rica com uma distribui√ß√£o te√≥rica parametrizada. Um exemplo disso pode ser testar se o valor do saldo da conta no treinamento (distribui√ß√£o A) segue uma distribui√ß√£o normal (distribui√ß√£o B).</p>
<h3 align="center">Distribui√ß√£o de Kolmogorov e Boa Adequa√ß√£o</h3>
<p>A estat√≠stica do teste K-S, que √© o valor alcan√ßado medindo a diferen√ßa m√°xima entre as duas eCDFs, representa apenas uma √∫nica amostra. A distribui√ß√£o de Kolmogorov √© a distribui√ß√£o das estat√≠sticas do teste K-S se voc√™ fosse pegar um grande n√∫mero de amostras da mesma distribui√ß√£o.</p>
<p>Para criar a distribui√ß√£o de Kolmogorov:</p>
<ul>
  <li>Voc√™ pode usar dados de inicializa√ß√£o ou reunir outra eCDF</li>
  <li>Execute 500 vezes para gerar 500 eCDFs</li>
  <li>Calcule o K-S para cada eCDF</li>
  <li>Trace todos os 500 valores de K-S como uma distribui√ß√£o</li>
</ul>
<img src="ks-measurements-example.webp">
<p>O que foi mencionado acima √© como a distribui√ß√£o do teste K-S se parece com tamanhos de amostra de N. Cada amostra que voc√™ coleta ter√° um valor de K-S diferente.</p>
<p>Andrey Kolmogorov observa em seu artigo original sobre o assunto que a distribui√ß√£o dos valores de K-S de m√∫ltiplas amostras adquire uma forma espec√≠fica, que √© agora chamada de distribui√ß√£o de Kolmogorov.</p>
<img src="ks-metric-auc-one.webp">
<p>Essa distribui√ß√£o possui uma equa√ß√£o de forma fechada que depende apenas do n√∫mero de amostras que foram usadas na gera√ß√£o da eCDF. Uma distribui√ß√£o de Kolmogorov √© diferente de uma distribui√ß√£o gaussiana, mas ainda √© poss√≠vel aplicar alguns dos mesmos conceitos de valor p. O valor p retornado em Python pelo "ks_2samp" est√° apenas analisando essa distribui√ß√£o com base no n√∫mero de amostras e na estat√≠stica do teste K-S medida. O c√≥digo determina a probabilidade do valor da estat√≠stica do teste K-S amostrada, assumindo que o K-S √© distribu√≠do como uma distribui√ß√£o de Kolmogorov. Isso √© o teste de boa adequa√ß√£o. Para obter detalhes adicionais, o StatsExchange oferece uma √≥tima an√°lise t√©cnica aprofundada sobre este t√≥pico.</p>
<h3 align="center">Como Monitorar o Teste K-S ao Longo do Tempo?</h3>
<p>No uso t√≠pico do teste K-S em livros de estat√≠stica, trata-se de uma compara√ß√£o est√°tica entre duas distribui√ß√µes A e B. No caso de uso de monitoramento, geramos eCDFs periodicamente, medindo K-S periodicamente. Acompanhamos a estat√≠stica do teste K-S ao longo do tempo como um indicador de que a distribui√ß√£o se moveu. O K-S e o teste de boa adequa√ß√£o s√£o usados como um teste em cada amostra para definir um limiar de alerta.</p>
<p>O diagrama acima ilustra o uso ativo do K-S no monitoramento de modelos de caracter√≠sticas, onde um teste √© executado diariamente em uma caracter√≠stica para garantir que ela esteja dentro da distribui√ß√£o.</p>
<p>Vale ressaltar que executar isso na pr√°tica em um ambiente operacional de aprendizado de m√°quina em larga escala n√£o √© t√£o simples quanto executar um teste de boa adequa√ß√£o diariamente em um trabalho. Quando ocorre um alerta, o trabalho est√° apenas come√ßando, frequentemente, descobrir rapidamente o que mudou e como isso est√° relacionado aos dados √© fundamental para gerenciar dezenas a milhares de modelos e mais de 100 caracter√≠sticas.</p>
<h1 align="center">M√©tricas de Avalia√ß√£o de Modelos de Regress√£o</h1>
<h2 align="center">R¬≤</h2>
<p>A m√©trica R¬≤, tamb√©m conhecida como <b>R-dois</b> ou <b>coeficiente de determina√ß√£o</b>, representa o percentual da vari√¢ncia dos dados que √© explicado pelo modelo. Os resultados variam de 0 a 1, geralmente tamb√©m s√£o expressos em termos percentuais, ou seja, variando entre 0% e 100%. Quanto maior √© o valor de R¬≤, mais explicativo √© o modelo em rela√ß√£o aos dados previstos. Na equa√ß√£o 2 √© mostrado o c√°lculo desta m√©trica, no qual y e ≈∑ os valores reais e previstos, respectivamente, e y-barra representa a m√©dia dos valores reais.</p>
<img src="R-dois.png">
<p>Por√©m, utilizando somente esta m√©trica, ser√° que um valor de 0,5 j√° seria o suficiente para colocar o modelo em produ√ß√£o? Faz-se necess√°rio aproximar ainda mais o seu resultado para 1? Para responder a essa e outras perguntas, deve-se sempre utilizar outras m√©tricas para se ter uma vis√£o global sobre a performance do modelo.</p>
<h2 align="center">Erro M√©dio Absoluto (MAE ‚Äî Mean Absolute Error)</h2>
<p>O erro m√©dio absoluto (MAE ‚Äî do ingl√™s Mean Absolute Error), como demonstrado na equa√ß√£o 3, mede a m√©dia da diferen√ßa entre o valor real com o predito. Mas por haver valores positivos e negativos, √© adicionado um m√≥dulo entre a diferen√ßa dos valores. Al√©m disso, esta m√©trica n√£o √© afetada por valores discrepantes ‚Äî os denominados outliers.</p>
<img src="MAE.png">
<p>O valor de sa√≠da da equa√ß√£o tem a mesma escala dos dados utilizados para previs√£o, logo fica mais f√°cil a sua interpreta√ß√£o. Se o valor de MAE resultante for igual a 10,01 m, por exemplo, este resultado significa que o modelo pode estar errando em m√©dia 10,01 m para mais quanto para menos em rela√ß√£o ao valor correto. Por isso que para uma previs√£o futura, este resultado precisa ser levado em considera√ß√£o para a tomada de decis√£o. Contudo o quanto este erro representa em rela√ß√£o ao valor real percentualmente?</p>
<h2 align="center">Erro Percentual Absoluto M√©dio (MAPE ‚Äî Mean Absolute Percentual Error)</h2>
<p>O erro percentual absoluto m√©dio (MAPE ‚Äî do ingl√™s Mean Absolute Percentual Error) √© uma m√©trica que mostra a porcentagem de erro em rela√ß√£o aos valores reais. Na equa√ß√£o 4 representa o c√°lculo de MAPE que basicamente se parece com MAE, mas com o acr√©scimo de uma divis√£o por |y|. Ent√£o se o resultado de MAPE for igual a 40% significa que o nosso modelo faz previs√µes que em m√©dia a diferen√ßa entre o valor previsto e o real equivale a 40% do valor real tanto para mais quanto para menos.</p>
<img src="MAPE.png">
<p>Ao observar a equa√ß√£o 4 nota-se que caso o valor de y seja 0, ocorrer√° um erro, devido a divis√£o por zero. Por isso que a biblioteca scikit-learn utiliza a tratativa de colocar um n√∫mero muito pequeno, representado por Œµ, cujo valor √© 2,220446049250313e-16. A m√©trica MAPE √© uma das m√©tricas mais usadas para reportar a performance do modelo, trazendo uma compreens√£o mais abrangente do resultado de MAE.</p>
<h2 align="center">Erro Quadr√°tico M√©dio (MSE ‚Äî Mean Squared Error)</h2>
<p>O erro quadr√°tico m√©dio (MSE ‚Äî do ingl√™s Mean Squared Error) √© uma m√©trica que calcula a m√©dia de diferen√ßa entre o valor predito com o real, como a m√©trica MAE. Entretanto, ao inv√©s de usar o m√≥dulo do resultado entre o valor de y e ≈∑, nesta m√©trica a diferen√ßa √© elevada ao quadrado. Desta maneira penalizando valores que sejam muito diferentes entre o previsto e o real. Portanto, quanto maior √© o valor de MSE, significa que o modelo n√£o performou bem em rela√ß√£o as previs√µes.</p>
<img src="MSE.png">
<p>Apesar de sua ideia poderosa, a m√©trica MSE apresenta um problema de interpretabilidade. Por haver a eleva√ß√£o ao quadrado, a unidade fica distorcida, em outras palavras, se a unidade medida for metros (m), o resultado ser√° em m¬≤. Por isso que uma adapta√ß√£o da MSE √© a RMSE que ser√° apresentada abaixo.</p>
<h2 align="center">Raiz do Erro Quadr√°tico M√©dio (RMSE ‚Äî Root Mean Squared Error)</h2>
<p>A raiz do erro quadr√°tico m√©dio (RMSE ‚Äî do ingl√™s, Root Mean Squared Error) √© basicamente o mesmo c√°lculo de MSE, contendo ainda a mesma ideia de penaliza√ß√£o entre diferen√ßas grandes do valor previsto e o real. Por√©m, para lidar com o problema da diferen√ßa entre unidades, √© aplicada a raiz quadr√°tica como demonstrado na equa√ß√£o 6. Assim a unidade fica na mesma escala que o dado original, resultando em uma melhor interpretabilidade do resultado da m√©trica.</p>
<img src="RMSE.png">
<p>Apesar do valor ter a mesma unidade, ele n√£o costuma se assemelhar ao resultado encontrado de MAE, demonstrando como os outliers podem estar impactando nas previs√µes do modelo. Mas a sua interpretabilidade pode seguir a mesma l√≥gica, onde o resultado da m√©trica sendo igual a 80,0 m, significa que o modelo pode estar errando em 80,0 m para mais ou para menos. Por essa raz√£o, esta m√©trica pode ser uma boa op√ß√£o quando √© preciso ter uma avalia√ß√£o mais criteriosa sobre as previs√µes do modelo.</p>
<h3 align="center">Refer√™ncias:</h3>
<p>Trevor Hastie, Robert Tibshirani, Jerome Friedman. 2001. The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc.</p>
<p>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.</p>
<p>Kai Ming Ting. 2011. Encyclopedia of machine learning. Springer. ISBN 978‚Äì0‚Äì387‚Äì30164‚Äì8.</p>
<p>https://medium.com/kunumi/m%C3%A9tricas-de-avalia%C3%A7%C3%A3o-em-machine-learning-classifica%C3%A7%C3%A3o-49340dcdb198</p>
<p>https://mariofilho.com/guia-completo-da-log-loss-perda-logaritmica-em-machine-learning/</p>
<p>https://medium.com/data-hackers/prevendo-n%C3%BAmeros-entendendo-m%C3%A9tricas-de-regress%C3%A3o-35545e011e70</p>
<p>https://arize.com/blog-course/kolmogorov-smirnov-test/#what-is-ks-test</p>
